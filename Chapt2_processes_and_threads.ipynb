{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Processes ##\n",
    "1. Process: an abstraction of a running program.\n",
    "\n",
    "2. Pseudoparallelism v.s. multiprocessor \n",
    "\n",
    "3. In each CPU, there is always one process running.\n",
    "\n",
    "### 2.1.1 The Process Model ###\n",
    "1. In this model, all the runnable software on the computer, sometimes including the operating system, is organized into a number of ***sequential processes(processes)***.\n",
    "\n",
    "2. A process is an instance of an executing program, including the current values of the program counter, registers, and variables. Conceptually, each process has its own virtual CPU. \n",
    "\n",
    "3. Processes must not be programmed with built-in assumptions about timing.\n",
    "\n",
    "4. Process v.s. program\n",
    "        - a ***process*** is an activity of some kind. It has a program, input, output, and a state.\n",
    "        - a single processor may be shared among several processes, with some scheduling algorithm being accustomed to determine when to stop work on one proces and service a different one.\n",
    "        - in contrast, a ***program*** is something that may be stored on disk, not doing anything.\n",
    "\n",
    "### 2.1.2 Process Creation ###\n",
    "1. Four principle events cause processes to e created:\n",
    "        - System initialization.\n",
    "        - Execution of a process-creation system call by a running process.\n",
    "        - A user request to create a new process.\n",
    "        - Initiaiton of a batch job.\n",
    "        \n",
    "2. Deamons: processes that stey in the background to handle some activity.\n",
    "\n",
    "3. Technically, in all cases, a new process is created by having an existing process execute a process creation system call.\n",
    "\n",
    "4. `fork` in the only one system call to create a new process in UNIX. This call creates an exact clone fo the calling process. After that, the child process executes `execve` or a similar system call to change its memory image and run a new program.\n",
    "\n",
    "5. In Windows, in contrast, s single Win32 function call, `CreateProcess`, handles both process creation and loading the correct program into the new process.\n",
    "\n",
    "6. In both UNIX and Windows systems, after a process is created, the parent and child have their own distinct address spaces. NO WRITABLE MEMORY IS SHARED.\n",
    "\n",
    "\n",
    "### 2.1.3 Process Termination ###\n",
    "1. Conditions where new process will terminate:\n",
    "        - Normal exit (voluntary). E.g. the process has done its work\n",
    "        - Error exit (voluntary). E.g. the process discovers a fatal error\n",
    "        - Fatal error (involuntary). E.g. caused by the process, bugs\n",
    "        - Killed by another process (involuntary). E.g. (by system call `kill`)\n",
    "        \n",
    "### 2.1.4 Process Hierarchies ###\n",
    "1. In some systems, when a process creates another process, the parent process and child process continue to be associated in certain ways. The child process can itself create more processes, forming a process hierarchy.\n",
    "\n",
    "2. A process has only one parent but can have multiple chilren.\n",
    "\n",
    "3. In UNIX, a process and all of its children and further descendants together form a process group.\n",
    "\n",
    "4. Unlike in Windows, process in UNIX cannot disinherit their children.\n",
    "\n",
    "### 2.1.5 Process States ###\n",
    "1. Three states a process may be in:\n",
    "        - Running (actually using the CPU at that instant).\n",
    "        - Ready (runnable; temporarily stopped to let another process run).\n",
    "        - Blocked (unable to run until some external event happends).\n",
    "\n",
    "2. Four possible transitions among these three states:\n",
    "        - Process blocks for input.\n",
    "        - Scheduler picks another paroacess.\n",
    "        - Scheduler picks this process.\n",
    "        - Input becomes available.\n",
    "    \n",
    "3. The lowest layer of a process-structured operating system handles interrupts and scheuling. Above taht layer are sequential processes.\n",
    "\n",
    "### 2.1.6 Implementation of Processes ###\n",
    "1. The operating system maintains a table (an array of structures), called the **process table/process control blocks**. This entry contains import information abotu the process' state, i.e., its program counter, stack pointer, memory allocation, the status of its open files, its accouting and scheduling information.\n",
    "\n",
    "2. Associated with each I/O class is a location (typically at a fixed location near the bottom of memory) called the **interrupt vector**. It contains the address of the interrupt service procedure. \n",
    "\n",
    "### 2.1.7 Modeling Multiprogramming ###\n",
    "1. Skeleton of what the lowest level of the operating system does when an interrupt occurs:\n",
    "        a. Hadware stacks program counter, etc.\n",
    "        b. Hardware loads new program counter from interrupt vector.\n",
    "        c. Assembly-language procedure saves registers.\n",
    "        d. Assembly-language procedure sets up new stack.\n",
    "        e. C interrupt service runs (typically reads and buffers input)\n",
    "        f. Scheduler decides which process is to run next.\n",
    "        g. C procedure returns to the assembly code.\n",
    "        h. Assembly-language procedure starts up new current process.\n",
    "        \n",
    "2. Degree of multiprogramming : don't quite get this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of using pop-up threads is that the latency between message arrival and the start of processing can be made very short.\n",
    "\n",
    "## 2.2 Threads ##\n",
    "In traditional operating systems, each process has an address space and a single ghread of control. In fact, thatis almost the definition of a process. Nevertheless, in many situations, it is desirable to have multiple threads of control in the same address space running in quasi-parallel, as though they were (almost) separate processes (except for the shared address space).\n",
    "\n",
    "### 2.2.1 Thread Usage ###\n",
    "1. Threads: miniprocesses running in a process\n",
    "\n",
    "2. Reasons to have threads:\n",
    "        - In many aplications, multiple activities are going on at once. Some of these may block from time to time.\n",
    "        - Threads have the ability for the parallel entities to share an address space and all of its data among themselves.\n",
    "        - Threads are lighter weight than processes, thus they are easier (i.e., faster) to create and destroy.\n",
    "        - Threads yield no performance gain when all of them are CPU bound, but when there is substantial computing and also substantial I/O, having threads allows these activities to overlap, thus speeding up the application.\n",
    "        - Threads are useful on systems with multiple CPUs, where real parallelism is possible.\n",
    "        \n",
    "3. Example: Word processors: one thread listening to user input, one thread reformatting files, and one backup the whole file periodically.\n",
    "\n",
    "4. Finite-state machine: each computation has a saved state, and there exists some set or events that can occur to change the state.\n",
    "\n",
    "5. Three ways to construct a server:\n",
    "        **Model**                  **Characteristics**\n",
    "        Threads                     Parallelism, blocking system calls\n",
    "        Single-threaded process     No parallelism, blocking system calls\n",
    "        Finite-state machine        Parallelism, nonblocking system calls, interrupts\n",
    "\n",
    "6. Example: an application that processes very large amounts of data. One input thread, one processing thread, one output thread.\n",
    "\n",
    "### 2.2.2 The Classical Tread Model ###\n",
    "1. The process model is based on two independent concepts: resource grouping and execution.\n",
    "\n",
    "2. One way of looking at a process is that it is a way to group related resources together. A process has an address space containing program text and data, as well as other resources. These resources may include open files, child processes, pending alarms, signal handlers, accouting information, and more. By putting them together in the form of a process, they can be managed more easily.\n",
    "\n",
    "3. The other concept a process has is a thread of execution. Process are used to group resources together, threads are the entities scheduled for execution on the CPU.\n",
    "\n",
    "4. What treads add to the process model is to allow multiple executions to take place in the same process enviroment, to a large degree independent of one another.\n",
    "\n",
    "5. When a multithreaded process is run on a single-CPU system, the threads take turns running.\n",
    "\n",
    "6. Items shared by all threads in a process and items private to each thread:\n",
    "        **Per-process items**            **Per-thread items**\n",
    "        Address space                    Program counter\n",
    "        Global variables                 Registers\n",
    "        Open files                       Stack\n",
    "        Child processes                  State\n",
    "        Pending alarms\n",
    "        Signals and signal handlers\n",
    "        Accouting information\n",
    "        \n",
    "7. States of a thread: running; blocked; ready; terminated\n",
    "\n",
    "8. `thread_create`. When multithreading is present, processes usually start with a single thread present. This ghread ha sthe ability to create new threads by calling this library procedure.\n",
    "\n",
    "9. `thread_exit`. When a tread finishes its work.\n",
    "\n",
    "10. `thread_join`. One thread can wait for a (specific) thread to exit by calling this procedure.\n",
    "\n",
    "11. `thread_yield`. Allows a thread to voluntarily give up the CPU to let another thread run.\n",
    "\n",
    "### 2.2.3 POSIX Threads ###\n",
    "1. Pthreads: To make it possible to write portable threaded programs, IEEE has defined a standard to threads in IEEE standard 1003.1c. The threads package it defines is called **Pthreads**.\n",
    "\n",
    "### 2.2.4 Implementing Threads in User Space ###\n",
    "1. Advantages: \n",
    "        - A user-level threads package can be implemented on an operating system that does not support threads.\n",
    "        - They allow each process to have its own customized scheduling algorithm.\n",
    "\n",
    "2. When threads are managed in user space, each process needs it own private **thread table** to keep track of the threads in that process.\n",
    "\n",
    "3. If the program calls or jumps to an instruction that is not in memory, a **page fault** occurs and the operating system will go and get the missing instruction (and its neighbors) from disk.\n",
    "\n",
    "4. Disavantages:\n",
    "        - The problem of hwo blocking system calls are implemented.\n",
    "        - If a thread starts runing, no other thread in that process will ever run unless the first thread voluntarily gives up the CPU.\n",
    "        - Programmers generally want threads precisely in application where the threads block often, as, for example, in a multithreaded Web server.\n",
    "\n",
    "\n",
    "### 2.2.5 Implementing Threads in the Kernel ###\n",
    "1. The kernel's thread table holds each thread's registers, state, and other information.\n",
    "\n",
    "2. All calls that might block a thead are implemented as system calls, at consideraly greater cost than a call to a run-time system procedure.\n",
    "\n",
    "3. When a thread blocks, the kernel, can run either another thread from the same process (if one is ready) or a thread from a different process.\n",
    "\n",
    "4. Problems kernel threads can't solve perfectly:\n",
    "        - What happens when a multithreaded process forks?\n",
    "        - When a signal comes in (signals are sent to processes, not to threads), which thread should handle it?\n",
    "        \n",
    "### 2.2.6 Hybrid Implementation ###\n",
    "Use kernel-level threads and then multiplex user-level threads onto some or all of them. \n",
    "\n",
    "### 2.2.7 Scheduler Activations ###\n",
    "1. Approach divised by Anderson et al. (1988). The goals of the scheduler activation work are to mimic the functionality of kernal threads, but with the better performance and greater flexibility usually associated with threads packages implemented in user space.\n",
    "\n",
    "2. User threads should not have to make special nonblocking system calls or check in advance if it is safe to make certain system calls. Nevertheless, when a thread blocks on a system call or on a page fault, it should be possible to run other threads within the same process.\n",
    "\n",
    "3. When scheduler activations are used, the kernel assigns a certain number of virtual processors to each process and lets the (user-space) run-time system allocate threads to processors. \n",
    "\n",
    "4. upcall\n",
    "\n",
    "### 2.2.8 Pop-Up Threads ###\n",
    "1. In the example of requests for service. The arrival of a message causes the system to create a new thread to handle the message. Such a thread is called **pop-up thread**.\n",
    "\n",
    "2. The result of using pop-up threads is that the latency between message arrival and the start of processing can be made very short.\n",
    "\n",
    "### 2.2.9 Making Single-Threaded Code Multithreaded ###\n",
    "1. Problems need consideration:\n",
    "        - private global variables\n",
    "        - many library procedures are not reentrant (meaning they are not designed to have a second call made to any given procedure while a previous call has not yet finished\n",
    "        - some signals are logically thread specific, whereas others are not\n",
    "        - stack management (when a process' stack overflows, the kernel just provides that process with more stack automatically. When a process has multiple threads, it must also have multiple stacks. If the kernel is not aware of all these stacks, it cannot grow automatically upon stack falt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Interprocess Communication ##\n",
    "1. InterProcess Communication, also called IPC.\n",
    "\n",
    "2. Three main issues:\n",
    "        - how one process can pass information to another\n",
    "        - how to make sure two or more processes do not get in each other's way\n",
    "        - how to deal with proper sequencing when dependencies are present\n",
    "\n",
    "3. The above issues and solutions also apply to threads.\n",
    "\n",
    "### 2.3.1 Race Conditions ###\n",
    "1. ***Race conditions***: situations where two or more processes are reading or writing some shared data and the final result depends on who runs precisely when.\n",
    "\n",
    "2. Example: print spooler, where process A and B wants to write in the next free slot. Just the minute process A reads and stores the value, a clock interrupt occurs and the CPU decides A has run long enough and witches to process B. Process B read and write into the slot and erase what process A has written.\n",
    "\n",
    "### 2.3.2 Critical Regions ###\n",
    "1. ***Mutual exclusion***: some way of making sure that if one process is using a shared variable or file, the other process will be excluded from doing the same thing.\n",
    "\n",
    "2. ***Critical region/critical section***: the part of the program where the shared memory is accessed.\n",
    "\n",
    "3. To aviod condition races, we have to make sure no two processes are in their critical regions at the same time.\n",
    "\n",
    "4. A good solution for race conditions meets the following 4 requirements:\n",
    "        - No two processes may be simultaneously inside their critical regions.\n",
    "        - No assumptions may be made about speeds or the number of CPUs.\n",
    "        - No process running outside its critical region may block any process.\n",
    "        - No process should have to wait forever to enter its critical region.\n",
    "        \n",
    "### 2.3.3 Mutual Exclusion with Busy Waiting ###\n",
    "This section examines various proposals for achieving mutual exclusion.\n",
    "\n",
    "#### Disabling Interrupts ####\n",
    "1. Definition: to have each process disable all interrupts just after entering its critical region and re-enable them just before leaving it.\n",
    "\n",
    "2. Disabling intterupts is often a useful technique within the operating system itself but is not appropriate as a general mutual exlusion mechanism for user processes. \n",
    "\n",
    "3. However, this method won't work because \n",
    "        - situations like one process who disables interrupts but never turn them on again will crash the whole system\n",
    "        - for multiprocessor, disabling interrupts ONLY affects the CPU that executed the `disable` instruction\n",
    "        \n",
    "#### Lock Variables ####\n",
    "1. Definition: having a single, shared (lock) variable, initially 0 and is set to 1 when it is being read by a process.\n",
    "\n",
    "2. This won't work at all.\n",
    "\n",
    "#### Strict Alternation ####\n",
    "1. ***busy waiting***: Continuously testing a variable until some value appears. It should usually be avioded, since it watstes CPU time. Only tehre is a reasonable expectation taht the wait will be short is busy waiting used.\n",
    "\n",
    "2. ***spin lock***: A lock that uses busy waiting.\n",
    "\n",
    "3. Definition: there is integer variable `turn` to keeps track of whose turn it is to enter the critical region. \n",
    "\n",
    "```C\n",
    "/* Process 0 */\n",
    "while (TRUE) {\n",
    "    while (turn != 0)  /* loop */\n",
    "    critical_region();\n",
    "    turn = 1;\n",
    "    noncritical_region();\n",
    "}\n",
    "```\n",
    "\n",
    "```C\n",
    "/* Process 1 */\n",
    "while (TRUE) {\n",
    "    while (turn != 1)  /* loop */\n",
    "    critical_region();\n",
    "    turn = 0;\n",
    "    noncritical_region();\n",
    "}\n",
    "```\n",
    "\n",
    "4. However, process running outside its critical region may block other process.\n",
    "\n",
    "#### Peterson's Solution ####\n",
    "<img src=\"https://img.vim-cn.com/d2/53ae49c6d0debdeede0eccc6fe4879c8e6edc8.png\">\n",
    "\n",
    "\n",
    "#### The TSL Instruction ####\n",
    "1. `TSL RX.LOCK`(Test and Set Lock): it reads the contents of the memory wordk *lock* into register RX and tehm stores a nonzero value at the memory address *lock*. The operations of reading the word and storing into it are guaranteed to be indivisible - no other processor can access the memory word until the instruction is finished. The CPU executing the TSL instruction *locks the memory bus* to prohibit other CPUs from accessing memory until it is done.\n",
    "\n",
    "\n",
    "3. Entering and leaving a critical region using the TSL instruction:\n",
    "        enter_region:\n",
    "            TSL REGISTER,LOCK      |copy lock to register and set \n",
    "            CMP REGISTER,#0        |was lock zero? \n",
    "            JNE enter_region       |if it was not zero, lock was set, so loop\n",
    "            RET                    |return to caller; critical region entered\n",
    "        leave_region:\n",
    "            MOVE LOCK,#0           |store a 0 in lock\n",
    "            RET                    |return to caller\n",
    "            \n",
    "4. XCHG: an alternative instruction to TSL which exchanges the contents of two locations atomically and is used by all Intel x86 CPUs.\n",
    "\n",
    "\n",
    "### 2.3.4 Sleep and Wakeup ###\n",
    "1. ***priority inversion problem***: A scenario in scheduling in which a high priority task is indirectly preempted by a low priority task effectively inverting the relative priorities of the two tasks.\n",
    "\n",
    "2. The producer-consumer problem\n",
    "        - wakeup waiting bit\n",
    "\n",
    "### 2.3.5 Semaphores ###\n",
    "1. Definition: proposed by E. W. Dijkstra (1965), is an iteger variable to count the number of wakeups saved for future use. \n",
    "\n",
    "2. A semaphore has two operations: `down` and `up`. `down` checks if the value is greater than 0 and if so, it decrements the value and then continues. It the value is already 0, the process is put to sleep. `up` increment the value to 1.\n",
    "\n",
    "3. It is guaranteed that once a sempahore operation has started, no other process can access the semaphore until the operation has completed or blocked. It's called ***atomic action***. The operation of incrementing the semaphore and waking up one process is also atomic.\n",
    "\n",
    "4. We can use semaphores to solve the producer-consumer problem.\n",
    "\n",
    "```C\n",
    "#define N 100         /* number of slots in th buffer */\n",
    "typedef int semaphore;/* a special kind of int */\n",
    "semaphore mutex = 1;  /* control access to critical region */\n",
    "semaphore empty = N;  /* counts empty buffer slots */\n",
    "semaphore full = 0;   /* counts full buffer slots */\n",
    "\n",
    "void producer(void)\n",
    "{\n",
    "    int item;\n",
    "    while (TRUE) {\n",
    "        item = produce_item();\n",
    "        down(&empty);\n",
    "        down(&mutex);\n",
    "        insert_item(item);\n",
    "        up(&mutex);\n",
    "        up(&full);\n",
    "    }\n",
    "}\n",
    "\n",
    "void consumer(void)\n",
    "{\n",
    "    int item;\n",
    "    while (TRUE) {\n",
    "        down(&full);\n",
    "        down(&mutex);\n",
    "        remove_item(item);\n",
    "        up(&mutex);\n",
    "        up(&empty);\n",
    "        consume_item(item);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 2.3.6 Mutexes ###\n",
    "1. A **mutex** is a shared variable that can be in one of two states: unlocked and locked. They are easy and efficient to implement, which makes them especially useful in thread packages that are implemented entirely in user space.\n",
    "\n",
    "2. Implementation of *mutex_lock* and *mutex_unlock*\n",
    "\n",
    "```C\n",
    "mutext_lock:\n",
    "    TSL REGISTER,MUTEX\n",
    "    CMP REGISTER,#0\n",
    "    JZE ok\n",
    "    CALL thread_yield\n",
    "    JMP mutex_lock\n",
    "ok: RET\n",
    "\n",
    "mutex_unlock:\n",
    "    MOVE MUTEX,#0\n",
    "    RET\n",
    "```\n",
    "\n",
    "3. Difference between `enter_region` and `mutex_lock`:\n",
    "        - There is no busy waiting in `mutex_lock` because there is no clock that stops threads that have run too long; while in process, there is a clock and scheduler for each run of the process.\n",
    "        - When a thread fails to acquire a lock, it calls `thread_yield` to give up the CPU to another thread.\n",
    "        - Neither `mutex_lock` or `mutex_unlock` requires ay kernel call.\n",
    "\n",
    "#### Futexes ####\n",
    "1. Futex, \"fast user space mutex\", is a feature of Linux that implements basic locking (much like a mutex) but avoids dropping into the kernel unless it really has to, since switching to the kernel and back is quite expensive.\n",
    "\n",
    "#### Mutexes in Pthreads ####\n",
    "1. Some of the Pthreads calls relating to mutexes\n",
    "        `Pthread_mutex_init`    |  Create a mutex\n",
    "        `Pthread_mutex_destory` |  Destory an exiting mutex\n",
    "        `Pthread_mutex_lock`    |  Acquire a lock or block\n",
    "        `Pthread_mutex_trylock` |  Acquire a lock or fail\n",
    "        `Pthread_mutex_unlock`  |  Release a lock\n",
    "        \n",
    "2. ***Condition variables*** allow threads to lock due to some conditions not being met. They also allow the threads waiting and blocking to be done atomically.\n",
    "\n",
    "3. Condition variables and mutexes are always used together. For one thread to lock a mutex, then wait on a conditional variable when it cannot get what it needs. Eventually another thread will signal it and ti can continue.\n",
    "\n",
    "4. Condition variables (unlike semaphores) have no memory.\n",
    "\n",
    "5. Some of the Pthreads calls relating to condition variables\n",
    "        `Pthread_cond_init`\n",
    "        `Pthread_cond_destory`\n",
    "        `Pthread_cond_wait`      |  Block waiting for a signal\n",
    "        `Pthread_cond_signal`    |  Signal another thread and wake it up\n",
    "        `Pthread_cond_broadcast` |  Signal multiple threads and wake all of them\n",
    "        \n",
    "### 2.3.7 Monitors ###\n",
    "1. Brinch Hansen(1973) and Hoare (1974) proposed a higher-level synchronization primitive called a ***monitor***. \n",
    "\n",
    "2. A monitor is a collection of procedures, variables, and data structures taht are all grouped together in a special kind of mudle or package. It's a language concept.\n",
    "\n",
    "3. Processes may call the procedures in a monitor whenever they want to, buthey cannot directly access the monitor's internal data structures from procedures declared outside the monitor.\n",
    "\n",
    "4. Only one process can be active in a monitor at any isntant. By turning all the critical regions into monitor procedures, no two processes will ever execute their critical regions at the same time.\n",
    "\n",
    "5. By adding the keyword `synchronized` to a method declaration, Java guarantees that once any thread has started executing that method, no other thread will be allowed to start executing any other `synchronized` method.\n",
    "\n",
    "6. Semaphores are too low level and monitors are not usable except in a few programming languages. Also, none of the primitives allow information exchange between machines.\n",
    "\n",
    "### 2.3.8 Message Passing###\n",
    "1. Uses two primitives, `send` and `receive` system calls.\n",
    "\n",
    "2. Message passing is commonly used in parallel programming systes. \n",
    "\n",
    "3. MPI (Message-Passing Interface)\n",
    "\n",
    "### 2.3.9 Barriers ###\n",
    "1. Barriers are intended for groups of processes, in scenarios where aplications are divided into phrases and have a rule that no process may proceed into the next phrase until all processes are ready to proceed to the next phrase. A barrier is often placed at the end of each phrase.\n",
    "\n",
    "### 2.3.10 Avoiding Locks: Read-Copy-Update ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Scheduling ##\n",
    "1. Many of the same issues that apply to process scheduling also apply to thread scheduling but some are different.\n",
    "\n",
    "2. When the kernel manages threads, scheduling is usually done per thread, with little or or regard to which process the thread belongs.\n",
    "\n",
    "### 2.4.1 Introduction to Scheduling ###\n",
    "1. Most programs for personal computers are limited by the rate at which the user can present input (by typing or clicking), not by the rate the CPU can process it.\n",
    "\n",
    "2. In addition to picking the right process to run, the scheduler also has to worry about making efficient use of the CPU because process switching is expensive.\n",
    "\n",
    "#### Process Behavior ####\n",
    "1. I/O is when a process enters the blocked state waiting for an external device to complete its work.\n",
    "\n",
    "2. Compute-bound processes typically have long CPU bursts and thus infrequent I/O waits, whereas I/O-bound processes have short CPU bursts and thus frequent I/O bound. **The key factor is the length of the CPU bursts**.\n",
    "\n",
    "#### When to Schedule ####\n",
    "1. Nonpreemptive scheudling algorithm: picks a process to run and thenjsut lets it run until it blocks (either on I/O or waiting for another process) or voluntarily releases the CPU.\n",
    "\n",
    "2. Preemptive scheduling algorithm: picks a process and lets it run for a maximum of some fixed time.\n",
    "\n",
    "#### Categories of Scheduling Algorighm ####\n",
    "1. Batch systems usually use nonpreemptive algorithms.\n",
    "\n",
    "2. Environments with interative users, especially servers, often use preemptive algorithms.\n",
    "\n",
    "3. Real time environments often use nonpreemptive algorithms.\n",
    "\n",
    "#### Scheduling Algorithm Goals ####\n",
    "        All systems\n",
    "            - Fairness - giving each process a fair share of the CPU\n",
    "            - Policy enforcement - seeing that stated policy is carried out\n",
    "            - Balance - keeping all parts of the system busy\n",
    "        \n",
    "        Batch systems\n",
    "            - Throughput - maximize jobs per hour\n",
    "            - Turnaround time - minimize time between submission and termination\n",
    "            - CPU utilization - keep the CPU busy all the time\n",
    "            \n",
    "        Interactive systems\n",
    "            - Response time - respond to requests quickly\n",
    "            - Proportionality - meet users' expections\n",
    "        \n",
    "        Real-time systems\n",
    "            - Meeting deadlines - aviod losing data\n",
    "            - Predictability - avoid quality degradation in multimedia systems\n",
    "            \n",
    "### 2.4.2 Scheduling in Batch Systems ###\n",
    "#### First-Come, First-Served #### \n",
    "1. Advantage: easy to understand; easy to program\n",
    "\n",
    "2. Disadvantage: can't efficient deal with large amout of I/O bound processes\n",
    "\n",
    "3. Data structure: linked list\n",
    "\n",
    "#### Shortest Job First ####\n",
    "1. Provided there are four jobs: a, b, c, d and their runtime are 8, 4, 4, 4, respectively. Runing them in this order will produce a average turnaround time of 14 minutes. But we run these task in order b, c, d, a, the average turnaround time will be 11 minutes.\n",
    "\n",
    "2. This algorithm is optimal ONLY when all the jobs are available simultaneously.\n",
    "\n",
    "#### Shortest Remainign Time Next ####\n",
    "1. The run time has to be known in advance.\n",
    "\n",
    "\n",
    "### 2.4.3 Scheduling in Interactive Systems ###\n",
    "#### Round-Robin Scheduling ####\n",
    "1. The oldest, ismplest, fiarest, and most widely used algorithm.\n",
    "\n",
    "2. Each process is assigned a time interval, called its ***quantum***, during which it is allowed to run.\n",
    "\n",
    "3. Setting the quantum too short causes too many process switches and lowers the CPU efficiency, but setting it too long may cause poor reponse to short interactive requests. \n",
    "\n",
    "4. A quantum around 20-50 msec is often a reasonable compromise.\n",
    "\n",
    "#### Priority Scheduling ####\n",
    "1. Processes are given different size of quanta according to their priority. If they use up all quanta, they are moved down one class.\n",
    "\n",
    "2. The UNIX system has a command `nice`, which allows a user to voluntarily reduce the priority of his process, in order to be kind to the other users. Nobody ever uses it.\n",
    "\n",
    "#### Multiple Queues ####\n",
    "Processes are divided into different classes where each class has its own scheduling needs. For example, a common division is a foreground (interactive) process and background (batch) processes. These two classes have different scheduling needs.\n",
    "\n",
    "#### Shortest Process Next ####\n",
    "n/a\n",
    "\n",
    "#### Guaranteed Scheduling ####\n",
    "This algorithm garantees fairness by monitorin gthe amount of CPU time spent by each user and allocating resources accordingly.\n",
    "\n",
    "#### Lottery Scheduling ####\n",
    "1. Give processes lottery tickets for various system resources, such as CPU time. Whenever a scheduling decision has to be made, a lotter ticket is chosen at random, and the process holdin gthat ticket gets the resource.\n",
    "\n",
    "2. More important processes can be given extra tickets.\n",
    "\n",
    "3. A process holdinga fraction *f* of the tickets will get about a fraction of *f* of the resource in question.\n",
    "\n",
    "4. Cooperating processes may exchange tickets if they wish.\n",
    "\n",
    "#### Fair-Share Scheduling ####\n",
    "1. Each user is allocated some fraction of the CPU and the scheduler picks processes in such a way as to enforce it.\n",
    "\n",
    "2. If two users have each been promised 50% of the CPU, they will each get that, no matter how many processes they have in existence.\n",
    "\n",
    "### 2.4.4 Scheduling in Real-Time Systems ###\n",
    "1. Real-time systems are divided into \n",
    "        - hard real time (there are absolute deadlines that must be met\n",
    "        - soft real time (missing an occasional deadline is undesirable, but neverthelesse tolerable.\n",
    "\n",
    "2. The events that a real-time system may have to respond to can be further categorized as \n",
    "        - periodic (meaning they occur at regular intervals)\n",
    "        - aperiodic (they occur unpredictabley)\n",
    "        \n",
    "### 2.4.5 Policy Versus Mechanism ###\n",
    "1. None of the schedulers discussed above accept any input from user processes about schedulign decisions. However, it is not uncommon that one process has many children runnign under its control and the parent process should know best which its child is the most important.\n",
    "\n",
    "2. Seperating the ***scheduling mechanism*** from the ***scheduling policy*** is the key idea to solve the above question. The scheduling algorithm is parameterized in some way, but the parameters can be filled in by user processes.\n",
    "\n",
    "### 2.4.6 Thread Scheduling ###\n",
    "1. User-level threads vs. kernel-level threads\n",
    "\n",
    "2. A major different between user-level threads and kernel-level threads is the performance. Doing a thread switch with user-level threads takes a handlful of machine instructions. With kernel-level threads it requires a full context switch, changing the memory map and invalidating the cache, which is several orders of magnitude slower.\n",
    "\n",
    "3. On the other hand, with kernel-level threads, having a thread block on I/O does not suspend the entire process as it does with user-level threads.\n",
    "\n",
    "4. Another important factor is that user-level threads can employ an applicaiton-specific thread scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
