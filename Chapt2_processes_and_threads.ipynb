{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Processes ##\n",
    "1. Process: an abstraction of a running program.\n",
    "\n",
    "2. Pseudoparallelism v.s. multiprocessor \n",
    "\n",
    "3. In each CPU, there is always one process running.\n",
    "\n",
    "### 2.1.1 The Process Model ###\n",
    "1. In this model, all the runnable software on the computer, sometimes including the operating system, is organized into a number of ***sequential processes(processes)***.\n",
    "\n",
    "2. A process is an instance of an executing program, including the current values of the program counter, registers, and variables. Conceptually, each process has its own virtual CPU. \n",
    "\n",
    "3. Processes must not be programmed with built-in assumptions about timing.\n",
    "\n",
    "4. Process v.s. program\n",
    "        - a ***process*** is an activity of some kind. It has a program, input, output, and a state.\n",
    "        - a single processor may be shared among several processes, with some scheduling algorithm being accustomed to determine when to stop work on one proces and service a different one.\n",
    "        - in contrast, a ***program*** is something that may be stored on disk, not doing anything.\n",
    "\n",
    "### 2.1.2 Process Creation ###\n",
    "1. Four principle events cause processes to e created:\n",
    "        - System initialization.\n",
    "        - Execution of a process-creation system call by a running process.\n",
    "        - A user request to create a new process.\n",
    "        - Initiaiton of a batch job.\n",
    "        \n",
    "2. Deamons: processes that stey in the background to handle some activity.\n",
    "\n",
    "3. Technically, in all cases, a new process is created by having an existing process execute a process creation system call.\n",
    "\n",
    "4. `fork` in the only one system call to create a new process in UNIX. This call creates an exact clone fo the calling process. After that, the child process executes `execve` or a similar system call to change its memory image and run a new program.\n",
    "\n",
    "5. In Windows, in contrast, s single Win32 function call, `CreateProcess`, handles both process creation and loading the correct program into the new process.\n",
    "\n",
    "6. In both UNIX and Windows systems, after a process is created, the parent and child have their own distinct address spaces. NO WRITABLE MEMORY IS SHARED.\n",
    "\n",
    "\n",
    "### 2.1.3 Process Termination ###\n",
    "1. Conditions where new process will terminate:\n",
    "        - Normal exit (voluntary). E.g. the process has done its work\n",
    "        - Error exit (voluntary). E.g. the process discovers a fatal error\n",
    "        - Fatal error (involuntary). E.g. caused by the process, bugs\n",
    "        - Killed by another process (involuntary). E.g. (by system call `kill`)\n",
    "        \n",
    "### 2.1.4 Process Hierarchies ###\n",
    "1. In some systems, when a process creates another process, the parent process and child process continue to be associated in certain ways. The child process can itself create more processes, forming a process hierarchy.\n",
    "\n",
    "2. A process has only one parent but can have multiple chilren.\n",
    "\n",
    "3. In UNIX, a process and all of its children and further descendants together form a process group.\n",
    "\n",
    "4. Unlike in Windows, process in UNIX cannot disinherit their children.\n",
    "\n",
    "### 2.1.5 Process States ###\n",
    "1. Three states a process may be in:\n",
    "        - Running (actually using the CPU at that instant).\n",
    "        - Ready (runnable; temporarily stopped to let another process run).\n",
    "        - Blocked (unable to run until some external event happends).\n",
    "\n",
    "2. Four possible transitions among these three states:\n",
    "        - Process blocks for input.\n",
    "        - Scheduler picks another paroacess.\n",
    "        - Scheduler picks this process.\n",
    "        - Input becomes available.\n",
    "    \n",
    "3. The lowest layer of a process-structured operating system handles interrupts and scheuling. Above taht layer are sequential processes.\n",
    "\n",
    "### 2.1.6 Implementation of Processes ###\n",
    "1. The operating system maintains a table (an array of structures), called the **process table/process control blocks**. This entry contains import information abotu the process' state, i.e., its program counter, stack pointer, memory allocation, the status of its open files, its accouting and scheduling information.\n",
    "\n",
    "2. Associated with each I/O class is a location (typically at a fixed location near the bottom of memory) called the **interrupt vector**. It contains the address of the interrupt service procedure. \n",
    "\n",
    "### 2.1.7 Modeling Multiprogramming ###\n",
    "1. Skeleton of what the lowest level of the operating system does when an interrupt occurs:\n",
    "        a. Hadware stacks program counter, etc.\n",
    "        b. Hardware loads new program counter from interrupt vector.\n",
    "        c. Assembly-language procedure saves registers.\n",
    "        d. Assembly-language procedure sets up new stack.\n",
    "        e. C interrupt service runs (typically reads and buffers input)\n",
    "        f. Scheduler decides which process is to run next.\n",
    "        g. C procedure returns to the assembly code.\n",
    "        h. Assembly-language procedure starts up new current process.\n",
    "        \n",
    "2. Degree of multiprogramming : don't quite get this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of using pop-up threads is that the latency between message arrival and the start of processing can be made very short.\n",
    "\n",
    "## 2.2 Threads ##\n",
    "In traditional operating systems, each process has an address space and a single ghread of control. In fact, thatis almost the definition of a process. Nevertheless, in many situations, it is desirable to have multiple threads of control in the same address space running in quasi-parallel, as though they were (almost) separate processes (except for the shared address space).\n",
    "\n",
    "### 2.2.1 Thread Usage ###\n",
    "1. Threads: miniprocesses running in a process\n",
    "\n",
    "2. Reasons to have threads:\n",
    "        - In many aplications, multiple activities are going on at once. Some of these may block from time to time.\n",
    "        - Threads have the ability for the parallel entities to share an address space and all of its data among themselves.\n",
    "        - Threads are lighter weight than processes, thus they are easier (i.e., faster) to create and destroy.\n",
    "        - Threads yield no performance gain when all of them are CPU bound, but when there is substantial computing and also substantial I/O, having threads allows these activities to overlap, thus speeding up the application.\n",
    "        - Threads are useful on systems with multiple CPUs, where real parallelism is possible.\n",
    "        \n",
    "3. Example: Word processors: one thread listening to user input, one thread reformatting files, and one backup the whole file periodically.\n",
    "\n",
    "4. Finite-state machine: each computation has a saved state, and there exists some set or events that can occur to change the state.\n",
    "\n",
    "5. Three ways to construct a server:\n",
    "        **Model**                  **Characteristics**\n",
    "        Threads                     Parallelism, blocking system calls\n",
    "        Single-threaded process     No parallelism, blocking system calls\n",
    "        Finite-state machine        Parallelism, nonblocking system calls, interrupts\n",
    "\n",
    "6. Example: an application that processes very large amounts of data. One input thread, one processing thread, one output thread.\n",
    "\n",
    "### 2.2.2 The Classical Tread Model ###\n",
    "1. The process model is based on two independent concepts: resource grouping and execution.\n",
    "\n",
    "2. One way of looking at a process is that it is a way to group related resources together. A process has an address space containing program text and data, as well as other resources. These resources may include open files, child processes, pending alarms, signal handlers, accouting information, and more. By putting them together in the form of a process, they can be managed more easily.\n",
    "\n",
    "3. The other concept a process has is a thread of execution. Process are used to group resources together, threads are the entities scheduled for execution on the CPU.\n",
    "\n",
    "4. What treads add to the process model is to allow multiple executions to take place in the same process enviroment, to a large degree independent of one another.\n",
    "\n",
    "5. When a multithreaded process is run on a single-CPU system, the threads take turns running.\n",
    "\n",
    "6. Items shared by all threads in a process and items private to each thread:\n",
    "        **Per-process items**            **Per-thread items**\n",
    "        Address space                    Program counter\n",
    "        Global variables                 Registers\n",
    "        Open files                       Stack\n",
    "        Child processes                  State\n",
    "        Pending alarms\n",
    "        Signals and signal handlers\n",
    "        Accouting information\n",
    "        \n",
    "7. States of a thread: running; blocked; ready; terminated\n",
    "\n",
    "8. `thread_create`. When multithreading is present, processes usually start with a single thread present. This ghread ha sthe ability to create new threads by calling this library procedure.\n",
    "\n",
    "9. `thread_exit`. When a tread finishes its work.\n",
    "\n",
    "10. `thread_join`. One thread can wait for a (specific) thread to exit by calling this procedure.\n",
    "\n",
    "11. `thread_yield`. Allows a thread to voluntarily give up the CPU to let another thread run.\n",
    "\n",
    "### 2.2.3 POSIX Threads ###\n",
    "1. Pthreads: To make it possible to write portable threaded programs, IEEE has defined a standard to threads in IEEE standard 1003.1c. The threads package it defines is called **Pthreads**.\n",
    "\n",
    "### 2.2.4 Implementing Threads in User Space ###\n",
    "1. Advantages: \n",
    "        - A user-level threads package can be implemented on an operating system that does not support threads.\n",
    "        - They allow each process to have its own customized scheduling algorithm.\n",
    "\n",
    "2. When threads are managed in user space, each process needs it own private **thread table** to keep track of the threads in that process.\n",
    "\n",
    "3. If the program calls or jumps to an instruction that is not in memory, a **page fault** occurs and the operating system will go and get the missing instruction (and its neighbors) from disk.\n",
    "\n",
    "4. Disavantages:\n",
    "        - The problem of hwo blocking system calls are implemented.\n",
    "        - If a thread starts runing, no other thread in that process will ever run unless the first thread voluntarily gives up the CPU.\n",
    "        - Programmers generally want threads precisely in application where the threads block often, as, for example, in a multithreaded Web server.\n",
    "\n",
    "\n",
    "### 2.2.5 Implementing Threads in the Kernel ###\n",
    "1. The kernel's thread table holds each thread's registers, state, and other information.\n",
    "\n",
    "2. All calls that might block a thead are implemented as system calls, at consideraly greater cost than a call to a run-time system procedure.\n",
    "\n",
    "3. When a thread blocks, the kernel, can run either another thread from the same process (if one is ready) or a thread from a different process.\n",
    "\n",
    "4. Problems kernel threads can't solve perfectly:\n",
    "        - What happens when a multithreaded process forks?\n",
    "        - When a signal comes in (signals are sent to processes, not to threads), which thread should handle it?\n",
    "        \n",
    "### 2.2.6 Hybrid Implementation ###\n",
    "Use kernel-level threads and then multiplex user-level threads onto some or all of them. \n",
    "\n",
    "### 2.2.7 Scheduler Activations ###\n",
    "1. Approach divised by Anderson et al. (1988). The goals of the scheduler activation work are to mimic the functionality of kernal threads, but with the better performance and greater flexibility usually associated with threads packages implemented in user space.\n",
    "\n",
    "2. User threads should not have to make special nonblocking system calls or check in advance if it is safe to make certain system calls. Nevertheless, when a thread blocks on a system call or on a page fault, it should be possible to run other threads within the same process.\n",
    "\n",
    "3. When scheduler activations are used, the kernel assigns a certain number of virtual processors to each process and lets the (user-space) run-time system allocate threads to processors. \n",
    "\n",
    "4. upcall\n",
    "\n",
    "### 2.2.8 Pop-Up Threads ###\n",
    "1. In the example of requests for service. The arrival of a message causes the system to create a new thread to handle the message. Such a thread is called **pop-up thread**.\n",
    "\n",
    "2. The result of using pop-up threads is that the latency between message arrival and the start of processing can be made very short.\n",
    "\n",
    "### 2.2.9 Making Single-Threaded Code Multithreaded ###\n",
    "1. Problems need consideration:\n",
    "        - private global variables\n",
    "        - many library procedures are not reentrant (meaning they are not designed to have a second call made to any given procedure while a previous call has not yet finished\n",
    "        - some signals are logically thread specific, whereas others are not\n",
    "        - stack management (when a process' stack overflows, the kernel just provides that process with more stack automatically. When a process has multiple threads, it must also have multiple stacks. If the kernel is not aware of all these stacks, it cannot grow automatically upon stack falt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Interprocess Communication ##\n",
    "1. InterProcess Communication, also called IPC.\n",
    "\n",
    "2. Three main issues:\n",
    "        - how one process can pass information to another\n",
    "        - how to make sure two or more processes do not get in each other's way\n",
    "        - how to deal with proper sequencing when dependencies are present\n",
    "\n",
    "3. The above issues and solutions also apply to threads.\n",
    "\n",
    "### 2.3.1 Race Conditions ###\n",
    "1. ***Race conditions***: situations where two or more processes are reading or writing some shared data and the final result depends on who runs precisely when.\n",
    "\n",
    "2. Example: print spooler, where process A and B wants to write in the next free slot. Just the minute process A reads and stores the value, a clock interrupt occurs and the CPU decides A has run long enough and witches to process B. Process B read and write into the slot and erase what process A has written.\n",
    "\n",
    "### 2.3.2 Critical Regions ###\n",
    "1. ***Mutual exclusion***: some way of making sure that if one process is using a shared variable or file, the other process will be excluded from doing the same thing.\n",
    "\n",
    "2. ***Critical region/critical section***: the part of the program where the shared memory is accessed.\n",
    "\n",
    "3. To aviod condition races, we have to make sure no two processes are in their critical regions at the same time.\n",
    "\n",
    "4. A good solution for race conditions meets the following 4 requirements:\n",
    "        - No two processes may be simultaneously inside their critical regions.\n",
    "        - No assumptions may be made about speeds or the number of CPUs.\n",
    "        - No process running outside its critical region may block any process.\n",
    "        - No process should have to wait forever to enter its critical region.\n",
    "        \n",
    "### 2.3.3 Mutual Exlusion with Busy Waiting ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
